{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the environment\n",
    "env_name = 'CartPole-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the environment and extract the number of actions available in cartpole problem\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x7fa401db2438>>\n"
     ]
    }
   ],
   "source": [
    "# building single hidden layer neural network\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating policy and memory as sequencial memory as we are going to store action and respective reward\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "                                    target_model_update=1e-2, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   79/5000: episode: 1, duration: 3.733s, episode steps: 79, steps per second: 21, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.060 [-0.402, 0.722], loss: 0.426199, mean_absolute_error: 0.494559, mean_q: 0.054424\n",
      "  111/5000: episode: 2, duration: 0.539s, episode steps: 32, steps per second: 59, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.153 [-0.151, 0.650], loss: 0.350789, mean_absolute_error: 0.443192, mean_q: 0.187516\n",
      "  174/5000: episode: 3, duration: 1.060s, episode steps: 63, steps per second: 59, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.059 [-0.342, 0.786], loss: 0.312599, mean_absolute_error: 0.469253, mean_q: 0.336845\n",
      "  208/5000: episode: 4, duration: 0.571s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.081 [-0.228, 0.770], loss: 0.269011, mean_absolute_error: 0.508326, mean_q: 0.513763\n",
      "  260/5000: episode: 5, duration: 0.876s, episode steps: 52, steps per second: 59, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-0.352, 0.841], loss: 0.224247, mean_absolute_error: 0.560610, mean_q: 0.706505\n",
      "  295/5000: episode: 6, duration: 0.588s, episode steps: 35, steps per second: 59, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.095 [-0.204, 0.813], loss: 0.179460, mean_absolute_error: 0.636829, mean_q: 0.931348\n",
      "  328/5000: episode: 7, duration: 0.554s, episode steps: 33, steps per second: 60, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.076 [-0.348, 0.908], loss: 0.144498, mean_absolute_error: 0.710778, mean_q: 1.148125\n",
      "  354/5000: episode: 8, duration: 0.437s, episode steps: 26, steps per second: 60, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.396, 0.810], loss: 0.118843, mean_absolute_error: 0.792978, mean_q: 1.360441\n",
      "  375/5000: episode: 9, duration: 0.351s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.110 [-0.542, 0.947], loss: 0.096408, mean_absolute_error: 0.856442, mean_q: 1.534393\n",
      "  391/5000: episode: 10, duration: 0.274s, episode steps: 16, steps per second: 58, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.609, 1.077], loss: 0.089761, mean_absolute_error: 0.916874, mean_q: 1.700215\n",
      "  415/5000: episode: 11, duration: 0.398s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.627, 1.205], loss: 0.079219, mean_absolute_error: 0.972484, mean_q: 1.845982\n",
      "  436/5000: episode: 12, duration: 0.350s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.089 [-0.369, 1.041], loss: 0.085759, mean_absolute_error: 1.062691, mean_q: 2.040558\n",
      "  459/5000: episode: 13, duration: 0.388s, episode steps: 23, steps per second: 59, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.085 [-0.543, 1.001], loss: 0.098014, mean_absolute_error: 1.182673, mean_q: 2.225163\n",
      "  480/5000: episode: 14, duration: 0.352s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.083 [-0.576, 1.044], loss: 0.106641, mean_absolute_error: 1.250122, mean_q: 2.387394\n",
      "  495/5000: episode: 15, duration: 0.260s, episode steps: 15, steps per second: 58, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.084 [-0.608, 1.280], loss: 0.115834, mean_absolute_error: 1.316922, mean_q: 2.546251\n",
      "  508/5000: episode: 16, duration: 0.222s, episode steps: 13, steps per second: 59, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.109 [-0.594, 1.286], loss: 0.111727, mean_absolute_error: 1.377616, mean_q: 2.684163\n",
      "  524/5000: episode: 17, duration: 0.265s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.075 [-0.776, 1.451], loss: 0.140651, mean_absolute_error: 1.477399, mean_q: 2.836753\n",
      "  544/5000: episode: 18, duration: 0.324s, episode steps: 20, steps per second: 62, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.081 [-0.612, 1.265], loss: 0.167867, mean_absolute_error: 1.561927, mean_q: 2.983914\n",
      "  559/5000: episode: 19, duration: 0.267s, episode steps: 15, steps per second: 56, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.119 [-0.966, 1.823], loss: 0.188121, mean_absolute_error: 1.629128, mean_q: 3.136153\n",
      "  570/5000: episode: 20, duration: 0.167s, episode steps: 11, steps per second: 66, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.113 [-0.976, 1.608], loss: 0.184419, mean_absolute_error: 1.691507, mean_q: 3.237105\n",
      "  581/5000: episode: 21, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.146 [-0.771, 1.480], loss: 0.228391, mean_absolute_error: 1.731491, mean_q: 3.336565\n",
      "  593/5000: episode: 22, duration: 0.203s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.113 [-0.805, 1.493], loss: 0.172880, mean_absolute_error: 1.760817, mean_q: 3.432355\n",
      "  603/5000: episode: 23, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.135 [-0.756, 1.495], loss: 0.240661, mean_absolute_error: 1.849113, mean_q: 3.564191\n",
      "  615/5000: episode: 24, duration: 0.201s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.125 [-0.779, 1.497], loss: 0.261448, mean_absolute_error: 1.899015, mean_q: 3.683575\n",
      "  629/5000: episode: 25, duration: 0.237s, episode steps: 14, steps per second: 59, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.081 [-0.999, 1.669], loss: 0.249485, mean_absolute_error: 1.946526, mean_q: 3.765126\n",
      "  639/5000: episode: 26, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.120 [-0.776, 1.493], loss: 0.460570, mean_absolute_error: 2.079453, mean_q: 3.915332\n",
      "  651/5000: episode: 27, duration: 0.202s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.085 [-0.992, 1.574], loss: 0.305624, mean_absolute_error: 2.051821, mean_q: 3.948601\n",
      "  661/5000: episode: 28, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.116 [-1.228, 2.050], loss: 0.258237, mean_absolute_error: 2.078712, mean_q: 4.076583\n",
      "  675/5000: episode: 29, duration: 0.234s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.116 [-0.938, 1.671], loss: 0.472010, mean_absolute_error: 2.216704, mean_q: 4.246421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  687/5000: episode: 30, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.118 [-0.941, 1.699], loss: 0.338919, mean_absolute_error: 2.211718, mean_q: 4.248566\n",
      "  702/5000: episode: 31, duration: 0.268s, episode steps: 15, steps per second: 56, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.087 [-0.823, 1.424], loss: 0.431989, mean_absolute_error: 2.291770, mean_q: 4.382295\n",
      "  714/5000: episode: 32, duration: 0.183s, episode steps: 12, steps per second: 65, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.098 [-0.839, 1.536], loss: 0.365381, mean_absolute_error: 2.342580, mean_q: 4.506822\n",
      "  725/5000: episode: 33, duration: 0.185s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.122 [-0.958, 1.592], loss: 0.430653, mean_absolute_error: 2.403087, mean_q: 4.619078\n",
      "  735/5000: episode: 34, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.129 [-0.933, 1.664], loss: 0.435350, mean_absolute_error: 2.462561, mean_q: 4.725373\n",
      "  746/5000: episode: 35, duration: 0.195s, episode steps: 11, steps per second: 56, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.113 [-0.814, 1.465], loss: 0.495293, mean_absolute_error: 2.535158, mean_q: 4.827262\n",
      "  759/5000: episode: 36, duration: 0.215s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.102 [-1.193, 1.767], loss: 0.563414, mean_absolute_error: 2.571108, mean_q: 4.908652\n",
      "  771/5000: episode: 37, duration: 0.189s, episode steps: 12, steps per second: 63, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.114 [-1.161, 1.935], loss: 0.493957, mean_absolute_error: 2.605369, mean_q: 4.996356\n",
      "  780/5000: episode: 38, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.140 [-1.157, 1.952], loss: 0.525667, mean_absolute_error: 2.633458, mean_q: 5.093710\n",
      "  791/5000: episode: 39, duration: 0.186s, episode steps: 11, steps per second: 59, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.117 [-1.372, 2.124], loss: 0.604013, mean_absolute_error: 2.741346, mean_q: 5.237390\n",
      "  800/5000: episode: 40, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.128 [-1.381, 2.135], loss: 0.582410, mean_absolute_error: 2.737853, mean_q: 5.237849\n",
      "  808/5000: episode: 41, duration: 0.137s, episode steps: 8, steps per second: 58, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.157 [-1.175, 2.000], loss: 0.525441, mean_absolute_error: 2.781008, mean_q: 5.355126\n",
      "  817/5000: episode: 42, duration: 0.162s, episode steps: 9, steps per second: 55, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.129 [-1.613, 2.468], loss: 0.483905, mean_absolute_error: 2.806102, mean_q: 5.492530\n",
      "  827/5000: episode: 43, duration: 0.163s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.147 [-0.946, 1.733], loss: 0.513559, mean_absolute_error: 2.853903, mean_q: 5.579423\n",
      "  839/5000: episode: 44, duration: 0.195s, episode steps: 12, steps per second: 62, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.127 [-0.944, 1.648], loss: 1.087407, mean_absolute_error: 3.018673, mean_q: 5.622715\n",
      "  849/5000: episode: 45, duration: 0.162s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.141 [-0.943, 1.674], loss: 0.821924, mean_absolute_error: 3.035738, mean_q: 5.587630\n",
      "  858/5000: episode: 46, duration: 0.153s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.154 [-0.949, 1.733], loss: 0.599162, mean_absolute_error: 2.975465, mean_q: 5.653832\n",
      "  868/5000: episode: 47, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.138 [-0.942, 1.619], loss: 0.657434, mean_absolute_error: 3.059238, mean_q: 5.786477\n",
      "  877/5000: episode: 48, duration: 0.156s, episode steps: 9, steps per second: 58, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.109 [-1.401, 2.119], loss: 0.960367, mean_absolute_error: 3.140760, mean_q: 5.883459\n",
      "  887/5000: episode: 49, duration: 0.155s, episode steps: 10, steps per second: 64, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.126 [-1.186, 1.977], loss: 0.747584, mean_absolute_error: 3.117006, mean_q: 5.886636\n",
      "  900/5000: episode: 50, duration: 0.214s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.080 [-1.193, 1.745], loss: 0.979251, mean_absolute_error: 3.191526, mean_q: 5.900051\n",
      "  910/5000: episode: 51, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.122 [-1.144, 1.941], loss: 1.002054, mean_absolute_error: 3.225700, mean_q: 5.959853\n",
      "  922/5000: episode: 52, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.104 [-1.207, 1.828], loss: 0.843053, mean_absolute_error: 3.227588, mean_q: 6.030718\n",
      "  933/5000: episode: 53, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.108 [-1.215, 1.840], loss: 0.630475, mean_absolute_error: 3.210159, mean_q: 6.144449\n",
      "  943/5000: episode: 54, duration: 0.167s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.117 [-1.183, 1.944], loss: 0.951946, mean_absolute_error: 3.340378, mean_q: 6.269711\n",
      "  952/5000: episode: 55, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.129 [-1.349, 2.132], loss: 1.023432, mean_absolute_error: 3.344019, mean_q: 6.343573\n",
      "  965/5000: episode: 56, duration: 0.214s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.091 [-1.593, 2.427], loss: 0.938552, mean_absolute_error: 3.387706, mean_q: 6.376261\n",
      "  974/5000: episode: 57, duration: 0.153s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.775, 2.809], loss: 1.144192, mean_absolute_error: 3.445456, mean_q: 6.447965\n",
      "  986/5000: episode: 58, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.122 [-1.926, 3.095], loss: 1.048952, mean_absolute_error: 3.492543, mean_q: 6.498773\n",
      "  994/5000: episode: 59, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.605, 2.571], loss: 0.975498, mean_absolute_error: 3.502112, mean_q: 6.578044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1004/5000: episode: 60, duration: 0.170s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.123 [-1.608, 2.497], loss: 0.965405, mean_absolute_error: 3.526048, mean_q: 6.616965\n",
      " 1013/5000: episode: 61, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.127 [-1.773, 2.767], loss: 1.043108, mean_absolute_error: 3.549012, mean_q: 6.649749\n",
      " 1023/5000: episode: 62, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.130 [-1.754, 2.734], loss: 1.310334, mean_absolute_error: 3.636896, mean_q: 6.664918\n",
      " 1034/5000: episode: 63, duration: 0.208s, episode steps: 11, steps per second: 53, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.112 [-1.325, 2.017], loss: 0.527865, mean_absolute_error: 3.521692, mean_q: 6.712854\n",
      " 1046/5000: episode: 64, duration: 0.183s, episode steps: 12, steps per second: 66, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.122 [-1.131, 1.807], loss: 1.132222, mean_absolute_error: 3.667704, mean_q: 6.805613\n",
      " 1059/5000: episode: 65, duration: 0.212s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.088 [-0.995, 1.582], loss: 0.933189, mean_absolute_error: 3.676429, mean_q: 6.841831\n",
      " 1072/5000: episode: 66, duration: 0.234s, episode steps: 13, steps per second: 55, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.100 [-0.958, 1.447], loss: 0.892923, mean_absolute_error: 3.684959, mean_q: 6.860534\n",
      " 1085/5000: episode: 67, duration: 0.207s, episode steps: 13, steps per second: 63, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.100 [-0.989, 1.701], loss: 1.331784, mean_absolute_error: 3.801677, mean_q: 6.938454\n",
      " 1094/5000: episode: 68, duration: 0.139s, episode steps: 9, steps per second: 65, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.153 [-1.544, 2.499], loss: 1.106047, mean_absolute_error: 3.752978, mean_q: 6.923131\n",
      " 1104/5000: episode: 69, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.942, 3.066], loss: 1.172186, mean_absolute_error: 3.796224, mean_q: 7.045601\n",
      " 1114/5000: episode: 70, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.159 [-1.940, 3.084], loss: 1.114454, mean_absolute_error: 3.796917, mean_q: 7.028828\n",
      " 1122/5000: episode: 71, duration: 0.134s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.611, 2.526], loss: 1.136600, mean_absolute_error: 3.806965, mean_q: 7.008527\n",
      " 1134/5000: episode: 72, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.133 [-1.345, 2.326], loss: 0.933889, mean_absolute_error: 3.813010, mean_q: 7.148345\n",
      " 1145/5000: episode: 73, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.101 [-1.805, 2.807], loss: 1.480623, mean_absolute_error: 3.946671, mean_q: 7.167632\n",
      " 1153/5000: episode: 74, duration: 0.134s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.157 [-1.336, 2.211], loss: 0.995271, mean_absolute_error: 3.869714, mean_q: 7.121233\n",
      " 1162/5000: episode: 75, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.740, 2.796], loss: 0.908422, mean_absolute_error: 3.888954, mean_q: 7.174981\n",
      " 1174/5000: episode: 76, duration: 0.202s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.127 [-1.131, 1.872], loss: 0.982120, mean_absolute_error: 3.926135, mean_q: 7.259157\n",
      " 1186/5000: episode: 77, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.096 [-1.145, 1.688], loss: 0.948798, mean_absolute_error: 3.965768, mean_q: 7.367004\n",
      " 1197/5000: episode: 78, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.130 [-0.742, 1.262], loss: 1.218894, mean_absolute_error: 3.976808, mean_q: 7.212225\n",
      " 1209/5000: episode: 79, duration: 0.199s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.112 [-0.757, 1.462], loss: 1.110988, mean_absolute_error: 4.033427, mean_q: 7.364433\n",
      " 1223/5000: episode: 80, duration: 0.259s, episode steps: 14, steps per second: 54, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.097 [-0.940, 1.653], loss: 1.164607, mean_absolute_error: 4.058681, mean_q: 7.354356\n",
      " 1236/5000: episode: 81, duration: 0.194s, episode steps: 13, steps per second: 67, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.118 [-0.779, 1.461], loss: 1.058286, mean_absolute_error: 4.042335, mean_q: 7.392959\n",
      " 1248/5000: episode: 82, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.105 [-0.940, 1.623], loss: 1.005640, mean_absolute_error: 4.019327, mean_q: 7.337154\n",
      " 1263/5000: episode: 83, duration: 0.263s, episode steps: 15, steps per second: 57, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.095 [-0.806, 1.242], loss: 1.282892, mean_absolute_error: 4.109112, mean_q: 7.446189\n",
      " 1280/5000: episode: 84, duration: 0.270s, episode steps: 17, steps per second: 63, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.099 [-0.543, 1.001], loss: 0.991737, mean_absolute_error: 4.094169, mean_q: 7.492961\n",
      " 1308/5000: episode: 85, duration: 0.474s, episode steps: 28, steps per second: 59, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.099 [-0.406, 0.849], loss: 1.129231, mean_absolute_error: 4.149807, mean_q: 7.538911\n",
      " 1340/5000: episode: 86, duration: 0.537s, episode steps: 32, steps per second: 60, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.381, 1.061], loss: 1.001715, mean_absolute_error: 4.180614, mean_q: 7.717476\n",
      " 1387/5000: episode: 87, duration: 0.791s, episode steps: 47, steps per second: 59, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.066 [-0.780, 0.441], loss: 1.033910, mean_absolute_error: 4.279759, mean_q: 7.911483\n",
      " 1444/5000: episode: 88, duration: 0.959s, episode steps: 57, steps per second: 59, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.178 [-3.653, 3.654], loss: 0.949940, mean_absolute_error: 4.363510, mean_q: 8.100277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1493/5000: episode: 89, duration: 0.824s, episode steps: 49, steps per second: 59, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.087 [-0.630, 0.556], loss: 0.869173, mean_absolute_error: 4.497672, mean_q: 8.500762\n",
      " 1525/5000: episode: 90, duration: 0.538s, episode steps: 32, steps per second: 59, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.104 [-0.680, 0.246], loss: 0.853077, mean_absolute_error: 4.586233, mean_q: 8.669847\n",
      " 1553/5000: episode: 91, duration: 0.472s, episode steps: 28, steps per second: 59, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.111 [-0.704, 0.252], loss: 1.131083, mean_absolute_error: 4.758250, mean_q: 8.931754\n",
      " 1574/5000: episode: 92, duration: 0.347s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.097 [-0.773, 0.370], loss: 1.024432, mean_absolute_error: 4.831223, mean_q: 9.169635\n",
      " 1591/5000: episode: 93, duration: 0.290s, episode steps: 17, steps per second: 59, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.106 [-0.956, 0.586], loss: 1.388021, mean_absolute_error: 4.915623, mean_q: 9.247091\n",
      " 1609/5000: episode: 94, duration: 0.300s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.094 [-1.071, 0.389], loss: 1.408875, mean_absolute_error: 4.946934, mean_q: 9.243055\n",
      " 1632/5000: episode: 95, duration: 0.385s, episode steps: 23, steps per second: 60, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.045 [-0.864, 0.623], loss: 1.350268, mean_absolute_error: 5.024665, mean_q: 9.398918\n",
      " 1652/5000: episode: 96, duration: 0.337s, episode steps: 20, steps per second: 59, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.077 [-0.928, 0.451], loss: 1.434692, mean_absolute_error: 5.097295, mean_q: 9.502041\n",
      " 1667/5000: episode: 97, duration: 0.251s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.104 [-0.987, 0.394], loss: 1.395576, mean_absolute_error: 5.165269, mean_q: 9.700550\n",
      " 1685/5000: episode: 98, duration: 0.306s, episode steps: 18, steps per second: 59, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.069 [-1.130, 0.807], loss: 1.030824, mean_absolute_error: 5.171555, mean_q: 9.882550\n",
      " 1701/5000: episode: 99, duration: 0.265s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.090 [-1.227, 0.774], loss: 1.652515, mean_absolute_error: 5.245160, mean_q: 9.913503\n",
      " 1712/5000: episode: 100, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.141 [-2.324, 1.334], loss: 1.012262, mean_absolute_error: 5.187480, mean_q: 9.849445\n",
      " 1721/5000: episode: 101, duration: 0.148s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.145 [-2.281, 1.381], loss: 1.912814, mean_absolute_error: 5.410969, mean_q: 10.135744\n",
      " 1730/5000: episode: 102, duration: 0.153s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-2.915, 1.753], loss: 1.439318, mean_absolute_error: 5.290905, mean_q: 9.999694\n",
      " 1738/5000: episode: 103, duration: 0.133s, episode steps: 8, steps per second: 60, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.128 [-2.201, 1.379], loss: 2.041893, mean_absolute_error: 5.489236, mean_q: 10.238394\n",
      " 1748/5000: episode: 104, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-3.017, 1.955], loss: 2.026818, mean_absolute_error: 5.451891, mean_q: 10.170940\n",
      " 1757/5000: episode: 105, duration: 0.151s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.159 [-2.539, 1.550], loss: 1.728405, mean_absolute_error: 5.378309, mean_q: 10.115223\n",
      " 1766/5000: episode: 106, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-2.852, 1.755], loss: 1.486652, mean_absolute_error: 5.465475, mean_q: 10.416825\n",
      " 1775/5000: episode: 107, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-2.801, 1.776], loss: 1.561931, mean_absolute_error: 5.518516, mean_q: 10.500138\n",
      " 1784/5000: episode: 108, duration: 0.147s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.123 [-2.798, 1.805], loss: 1.490989, mean_absolute_error: 5.569442, mean_q: 10.575987\n",
      " 1795/5000: episode: 109, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-3.285, 2.182], loss: 1.459783, mean_absolute_error: 5.559574, mean_q: 10.570091\n",
      " 1805/5000: episode: 110, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-3.113, 1.958], loss: 2.659416, mean_absolute_error: 5.734899, mean_q: 10.674426\n",
      " 1817/5000: episode: 111, duration: 0.203s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.135 [-3.105, 1.944], loss: 1.716126, mean_absolute_error: 5.639758, mean_q: 10.655814\n",
      " 1826/5000: episode: 112, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.866, 1.782], loss: 2.038085, mean_absolute_error: 5.703234, mean_q: 10.755674\n",
      " 1836/5000: episode: 113, duration: 0.168s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-3.098, 1.981], loss: 3.468364, mean_absolute_error: 5.932549, mean_q: 10.930410\n",
      " 1851/5000: episode: 114, duration: 0.260s, episode steps: 15, steps per second: 58, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.108 [-1.389, 0.756], loss: 2.105192, mean_absolute_error: 5.793225, mean_q: 10.792868\n",
      " 1865/5000: episode: 115, duration: 0.228s, episode steps: 14, steps per second: 62, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.108 [-1.451, 0.757], loss: 1.792011, mean_absolute_error: 5.785707, mean_q: 10.844801\n",
      " 1878/5000: episode: 116, duration: 0.218s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.106 [-2.790, 1.733], loss: 2.135258, mean_absolute_error: 5.918800, mean_q: 11.069886\n",
      " 1887/5000: episode: 117, duration: 0.144s, episode steps: 9, steps per second: 62, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.155 [-2.357, 1.384], loss: 1.595234, mean_absolute_error: 5.804602, mean_q: 10.969580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1897/5000: episode: 118, duration: 0.168s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.132 [-2.564, 1.543], loss: 1.765942, mean_absolute_error: 5.940698, mean_q: 11.185366\n",
      " 1905/5000: episode: 119, duration: 0.142s, episode steps: 8, steps per second: 56, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-2.576, 1.578], loss: 2.274333, mean_absolute_error: 5.937957, mean_q: 11.172526\n",
      " 1915/5000: episode: 120, duration: 0.155s, episode steps: 10, steps per second: 65, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.123 [-2.596, 1.613], loss: 1.805741, mean_absolute_error: 5.920573, mean_q: 11.193787\n",
      " 1927/5000: episode: 121, duration: 0.204s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.135 [-1.330, 0.750], loss: 2.638311, mean_absolute_error: 5.991411, mean_q: 11.139003\n",
      " 1945/5000: episode: 122, duration: 0.302s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.071 [-1.195, 0.586], loss: 2.215339, mean_absolute_error: 5.998001, mean_q: 11.179255\n",
      " 1957/5000: episode: 123, duration: 0.201s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.130 [-1.199, 0.560], loss: 2.816004, mean_absolute_error: 6.102292, mean_q: 11.285004\n",
      " 1977/5000: episode: 124, duration: 0.335s, episode steps: 20, steps per second: 60, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.069 [-0.986, 0.593], loss: 2.022067, mean_absolute_error: 6.089736, mean_q: 11.365099\n",
      " 1995/5000: episode: 125, duration: 0.303s, episode steps: 18, steps per second: 59, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.086 [-1.160, 0.593], loss: 1.644215, mean_absolute_error: 6.014157, mean_q: 11.357412\n",
      " 2008/5000: episode: 126, duration: 0.217s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.125 [-1.397, 0.634], loss: 2.122775, mean_absolute_error: 6.082943, mean_q: 11.456568\n",
      " 2019/5000: episode: 127, duration: 0.180s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.105 [-1.474, 0.826], loss: 2.907108, mean_absolute_error: 6.245973, mean_q: 11.557641\n",
      " 2037/5000: episode: 128, duration: 0.304s, episode steps: 18, steps per second: 59, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.072 [-1.216, 0.827], loss: 2.120040, mean_absolute_error: 6.148207, mean_q: 11.528154\n",
      " 2064/5000: episode: 129, duration: 0.458s, episode steps: 27, steps per second: 59, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.068 [-1.094, 0.759], loss: 2.555737, mean_absolute_error: 6.209860, mean_q: 11.532635\n",
      " 2092/5000: episode: 130, duration: 0.459s, episode steps: 28, steps per second: 61, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.064 [-0.881, 0.374], loss: 2.244024, mean_absolute_error: 6.225000, mean_q: 11.597816\n",
      " 2111/5000: episode: 131, duration: 0.316s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.104 [-1.047, 0.392], loss: 2.215485, mean_absolute_error: 6.346784, mean_q: 11.944017\n",
      " 2131/5000: episode: 132, duration: 0.337s, episode steps: 20, steps per second: 59, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-0.962, 0.600], loss: 2.091500, mean_absolute_error: 6.386574, mean_q: 12.059656\n",
      " 2155/5000: episode: 133, duration: 0.405s, episode steps: 24, steps per second: 59, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-0.981, 0.438], loss: 2.173683, mean_absolute_error: 6.447382, mean_q: 12.213425\n",
      " 2173/5000: episode: 134, duration: 0.300s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.049, 0.615], loss: 3.239269, mean_absolute_error: 6.518779, mean_q: 12.137211\n",
      " 2191/5000: episode: 135, duration: 0.301s, episode steps: 18, steps per second: 60, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-0.870, 0.384], loss: 2.176644, mean_absolute_error: 6.473771, mean_q: 12.136147\n",
      " 2218/5000: episode: 136, duration: 0.452s, episode steps: 27, steps per second: 60, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.084 [-0.850, 0.354], loss: 2.312385, mean_absolute_error: 6.550892, mean_q: 12.320543\n",
      " 2237/5000: episode: 137, duration: 0.319s, episode steps: 19, steps per second: 60, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.101 [-0.926, 0.404], loss: 2.586002, mean_absolute_error: 6.593963, mean_q: 12.377915\n",
      " 2259/5000: episode: 138, duration: 0.371s, episode steps: 22, steps per second: 59, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-0.799, 0.421], loss: 2.776233, mean_absolute_error: 6.635632, mean_q: 12.377151\n",
      " 2287/5000: episode: 139, duration: 0.475s, episode steps: 28, steps per second: 59, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-0.921, 0.348], loss: 2.651810, mean_absolute_error: 6.643193, mean_q: 12.332866\n",
      " 2306/5000: episode: 140, duration: 0.322s, episode steps: 19, steps per second: 59, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.101 [-1.088, 0.416], loss: 3.194928, mean_absolute_error: 6.616785, mean_q: 12.161232\n",
      " 2327/5000: episode: 141, duration: 0.338s, episode steps: 21, steps per second: 62, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.088 [-1.041, 0.572], loss: 2.374377, mean_absolute_error: 6.738755, mean_q: 12.599176\n",
      " 2343/5000: episode: 142, duration: 0.271s, episode steps: 16, steps per second: 59, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.116 [-0.852, 0.386], loss: 2.389544, mean_absolute_error: 6.646498, mean_q: 12.507874\n",
      " 2373/5000: episode: 143, duration: 0.502s, episode steps: 30, steps per second: 60, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-0.803, 0.628], loss: 2.808526, mean_absolute_error: 6.800135, mean_q: 12.740295\n",
      " 2401/5000: episode: 144, duration: 0.466s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.097 [-0.848, 0.563], loss: 2.559205, mean_absolute_error: 6.823419, mean_q: 12.787545\n",
      " 2463/5000: episode: 145, duration: 1.047s, episode steps: 62, steps per second: 59, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.036 [-0.929, 0.446], loss: 2.306261, mean_absolute_error: 6.847187, mean_q: 12.920556\n",
      " 2491/5000: episode: 146, duration: 0.470s, episode steps: 28, steps per second: 60, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.131 [-0.790, 0.196], loss: 3.176117, mean_absolute_error: 7.073166, mean_q: 13.276537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2522/5000: episode: 147, duration: 0.519s, episode steps: 31, steps per second: 60, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.115 [-0.719, 0.361], loss: 2.154610, mean_absolute_error: 7.004448, mean_q: 13.284513\n",
      " 2574/5000: episode: 148, duration: 0.877s, episode steps: 52, steps per second: 59, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.046 [-0.731, 0.391], loss: 2.437306, mean_absolute_error: 7.091385, mean_q: 13.401912\n",
      " 2600/5000: episode: 149, duration: 0.435s, episode steps: 26, steps per second: 60, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.112 [-0.959, 0.592], loss: 2.196901, mean_absolute_error: 7.164228, mean_q: 13.608659\n",
      " 2635/5000: episode: 150, duration: 0.589s, episode steps: 35, steps per second: 59, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.115 [-0.742, 0.197], loss: 2.602726, mean_absolute_error: 7.233970, mean_q: 13.690705\n",
      " 2668/5000: episode: 151, duration: 0.555s, episode steps: 33, steps per second: 59, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.068 [-0.809, 0.393], loss: 2.794492, mean_absolute_error: 7.303520, mean_q: 13.760824\n",
      " 2693/5000: episode: 152, duration: 0.420s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.100 [-1.133, 0.368], loss: 2.912107, mean_absolute_error: 7.324075, mean_q: 13.758075\n",
      " 2721/5000: episode: 153, duration: 0.471s, episode steps: 28, steps per second: 59, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.083 [-0.693, 0.400], loss: 2.933661, mean_absolute_error: 7.385205, mean_q: 13.916196\n",
      " 2755/5000: episode: 154, duration: 0.573s, episode steps: 34, steps per second: 59, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-0.872, 0.185], loss: 3.738703, mean_absolute_error: 7.466972, mean_q: 13.930485\n",
      " 2788/5000: episode: 155, duration: 0.555s, episode steps: 33, steps per second: 59, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.088 [-0.873, 0.596], loss: 3.036959, mean_absolute_error: 7.461835, mean_q: 14.019676\n",
      " 2817/5000: episode: 156, duration: 0.487s, episode steps: 29, steps per second: 60, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.109 [-0.651, 0.191], loss: 3.356682, mean_absolute_error: 7.562149, mean_q: 14.181582\n",
      " 2863/5000: episode: 157, duration: 0.774s, episode steps: 46, steps per second: 59, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.073 [-0.862, 0.367], loss: 3.657198, mean_absolute_error: 7.565388, mean_q: 14.105478\n",
      " 2898/5000: episode: 158, duration: 0.591s, episode steps: 35, steps per second: 59, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.101 [-0.704, 0.403], loss: 3.137092, mean_absolute_error: 7.654378, mean_q: 14.406970\n",
      " 2957/5000: episode: 159, duration: 0.990s, episode steps: 59, steps per second: 60, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.076 [-0.809, 0.612], loss: 3.706424, mean_absolute_error: 7.715353, mean_q: 14.451414\n",
      " 2999/5000: episode: 160, duration: 0.705s, episode steps: 42, steps per second: 60, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.116 [-0.974, 0.285], loss: 3.878421, mean_absolute_error: 7.801283, mean_q: 14.600279\n",
      " 3122/5000: episode: 161, duration: 2.072s, episode steps: 123, steps per second: 59, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.189 [-0.474, 1.188], loss: 3.410220, mean_absolute_error: 7.813191, mean_q: 14.726439\n",
      " 3299/5000: episode: 162, duration: 2.982s, episode steps: 177, steps per second: 59, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.028 [-0.894, 0.790], loss: 3.166640, mean_absolute_error: 8.063002, mean_q: 15.286226\n",
      " 3361/5000: episode: 163, duration: 1.044s, episode steps: 62, steps per second: 59, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.135 [-0.762, 0.240], loss: 2.935800, mean_absolute_error: 8.232967, mean_q: 15.716848\n",
      " 3438/5000: episode: 164, duration: 1.294s, episode steps: 77, steps per second: 60, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.211 [-0.444, 1.128], loss: 2.830985, mean_absolute_error: 8.340886, mean_q: 15.934662\n",
      " 3531/5000: episode: 165, duration: 1.566s, episode steps: 93, steps per second: 59, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.079 [-0.747, 0.354], loss: 3.119666, mean_absolute_error: 8.527976, mean_q: 16.296841\n",
      " 3597/5000: episode: 166, duration: 1.112s, episode steps: 66, steps per second: 59, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.091 [-0.781, 0.339], loss: 2.982696, mean_absolute_error: 8.663813, mean_q: 16.604027\n",
      " 3676/5000: episode: 167, duration: 1.329s, episode steps: 79, steps per second: 59, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.097 [-0.895, 0.273], loss: 3.485228, mean_absolute_error: 8.864867, mean_q: 16.969374\n",
      " 3745/5000: episode: 168, duration: 1.160s, episode steps: 69, steps per second: 59, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.123 [-0.858, 0.451], loss: 3.320138, mean_absolute_error: 8.946019, mean_q: 17.082918\n",
      " 3787/5000: episode: 169, duration: 0.706s, episode steps: 42, steps per second: 59, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.144 [-0.726, 0.217], loss: 3.844057, mean_absolute_error: 9.043215, mean_q: 17.259398\n",
      " 3843/5000: episode: 170, duration: 0.942s, episode steps: 56, steps per second: 59, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.121 [-0.704, 0.413], loss: 3.217172, mean_absolute_error: 9.054163, mean_q: 17.314993\n",
      " 3901/5000: episode: 171, duration: 0.977s, episode steps: 58, steps per second: 59, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.145 [-0.773, 0.353], loss: 3.594732, mean_absolute_error: 9.199245, mean_q: 17.627085\n",
      " 3985/5000: episode: 172, duration: 1.413s, episode steps: 84, steps per second: 59, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.101 [-0.702, 0.480], loss: 3.844814, mean_absolute_error: 9.248138, mean_q: 17.629057\n",
      " 4033/5000: episode: 173, duration: 0.804s, episode steps: 48, steps per second: 60, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.138 [-0.678, 0.241], loss: 3.342990, mean_absolute_error: 9.374362, mean_q: 18.017061\n",
      " 4117/5000: episode: 174, duration: 1.415s, episode steps: 84, steps per second: 59, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.085 [-0.772, 0.389], loss: 3.706227, mean_absolute_error: 9.491839, mean_q: 18.185862\n",
      " 4231/5000: episode: 175, duration: 1.919s, episode steps: 114, steps per second: 59, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.030 [-0.884, 0.600], loss: 3.471868, mean_absolute_error: 9.597281, mean_q: 18.393078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4269/5000: episode: 176, duration: 0.639s, episode steps: 38, steps per second: 59, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.151 [-0.743, 0.372], loss: 3.244174, mean_absolute_error: 9.687693, mean_q: 18.653227\n",
      " 4320/5000: episode: 177, duration: 0.858s, episode steps: 51, steps per second: 59, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.076 [-1.203, 0.435], loss: 3.250243, mean_absolute_error: 9.667204, mean_q: 18.579103\n",
      " 4451/5000: episode: 178, duration: 2.207s, episode steps: 131, steps per second: 59, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.050 [-0.687, 0.374], loss: 3.353874, mean_absolute_error: 9.913673, mean_q: 19.080969\n",
      " 4522/5000: episode: 179, duration: 1.195s, episode steps: 71, steps per second: 59, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.066 [-0.817, 0.384], loss: 3.809139, mean_absolute_error: 10.106715, mean_q: 19.355053\n",
      " 4568/5000: episode: 180, duration: 0.774s, episode steps: 46, steps per second: 59, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.141 [-0.705, 0.211], loss: 3.476260, mean_absolute_error: 10.120472, mean_q: 19.430714\n",
      " 4631/5000: episode: 181, duration: 1.060s, episode steps: 63, steps per second: 59, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.157 [-0.893, 0.444], loss: 3.135144, mean_absolute_error: 10.247307, mean_q: 19.764923\n",
      " 4687/5000: episode: 182, duration: 0.943s, episode steps: 56, steps per second: 59, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.149 [-0.757, 0.169], loss: 3.172715, mean_absolute_error: 10.237666, mean_q: 19.708544\n",
      " 4743/5000: episode: 183, duration: 0.942s, episode steps: 56, steps per second: 59, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.153 [-0.767, 0.237], loss: 3.503192, mean_absolute_error: 10.309201, mean_q: 19.790972\n",
      " 4795/5000: episode: 184, duration: 0.874s, episode steps: 52, steps per second: 59, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.174 [-0.777, 0.195], loss: 3.223262, mean_absolute_error: 10.414174, mean_q: 20.097298\n",
      " 4916/5000: episode: 185, duration: 2.038s, episode steps: 121, steps per second: 59, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.074 [-0.884, 0.470], loss: 3.751605, mean_absolute_error: 10.554563, mean_q: 20.350683\n",
      "done, took 86.696 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa4001b3e80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot.\n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 54.000, steps: 54\n",
      "Episode 2: reward: 74.000, steps: 74\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
